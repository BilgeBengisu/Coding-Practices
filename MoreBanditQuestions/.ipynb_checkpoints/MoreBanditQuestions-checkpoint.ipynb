{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afac4fe7-7193-45ba-a591-fbfb326db809",
   "metadata": {},
   "source": [
    "# A Probability Problem and a Bandit Problem\n",
    "\n",
    "Write code in the cells indicated, and answer the questions as indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5adbcd5-0d4a-4a1e-860a-142c571c1ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef8eca-7744-42a5-b3e6-002eecbd60c6",
   "metadata": {},
   "source": [
    "### What Averaging Does to Noise\n",
    "\n",
    "Suppose $X_1(t), X_2(t), \\ldots, X_n(t) \\sim \\mathcal{N}(0, 1)$ are independent sources of independent Gaussian random variables, each producing a new random number every time step $t$.\n",
    "\n",
    "Each $X_i$ corresponds to a different gambler, and each $X_i(t)$ corresponds to that gambler's reward at time $t$. In this simplified setting, all the gamblers are playing the same machine.\n",
    "\n",
    "Plot $X_1(t)$ for times $t = 1, 2, \\ldots, 100$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aed3e07-7be9-4ddc-9979-19242732e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c959a-4f75-44f6-b6bd-ca975f4a8646",
   "metadata": {},
   "source": [
    "The plot illustrates the *variance* of $X_1 \\sim \\mathcal{N}(0, 1)$. Variance measures how much a random variable deviates from its mean. The usual formula is expected squared distance from the mean: $Var(X_1) = E(X_1 - EX_1)^2$. Since $X_1$ has mean $0$, $Var(X_1) = EX^2_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa7db9-6eb5-4bbe-9a3f-7d3aafcea47a",
   "metadata": {},
   "source": [
    "#### Variance of the average of independent random variables\n",
    "\n",
    "The average is \n",
    "$$\n",
    "\\bar{X} = \\frac{X_1 + \\cdots X_n}{n}\n",
    "$$\n",
    "\n",
    "Two of the rules of probability are\n",
    "1. *Linearity of expectation*: for any random variables $Y_1, \\ldots, Y_n$, $E(Y_1 + \\cdots + Y_n) = E(Y_1) + \\cdots + E(Y_n)$.\n",
    "2. *Independence*: for independent random variables $Y$ and $Z$, $E(YZ) = E(Y)E(Z)$.\n",
    "\n",
    "**Show** using these rules and algebra that\n",
    "$$\n",
    "Var(\\bar{X}) = \\frac{1}{n}Var(X_1).\n",
    "$$\n",
    "\n",
    "[If you get stuck, try showing this for $n=2$.]\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e69311-d95f-451e-bef4-58afe4a9f426",
   "metadata": {},
   "source": [
    "#### Plot of the average of independent random variables\n",
    "\n",
    "Choose $n=100$. \n",
    "\n",
    "Plot $\\bar{X}(t)$ in the next cell. Eyeball this plot and the previous plot. You may also go back and take statistics of the datasets plotted.\n",
    "\n",
    "**Question**: Do the plots confirm the calculation? Make your answer quantitative.\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc4ceb4-8ada-4a58-a464-aecc186c49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d57ff-d407-4d0f-ac89-535e6db85c3d",
   "metadata": {},
   "source": [
    "### What the UCBAgent Sees\n",
    "\n",
    "Visualize the numbers the UCBAgent uses to make its decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9800c7-b000-4b59-aa76-2ac1be3a78c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "#module_path = os.path.abspath('RlGlue') \n",
    "#sys.path.insert(0, module_path) \n",
    "\n",
    "from rlglue.rl_glue import RLGlue\n",
    "from main_agent import Agent\n",
    "from five_arm_env import Environment\n",
    "\n",
    "import jupyturtle as jt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe39b7-62a2-4b59-9090-215958117536",
   "metadata": {},
   "source": [
    "#### Some illustration of jupyturtle graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a68b7c5-3701-4f7f-be7e-c7613d776d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg width=\"800\" height=\"200\" style=\"fill:none; stroke-linecap:round;\">\n",
       "    <rect width=\"100%\" height=\"100%\" fill=\"#F3F3F7\" />\n",
       "\n",
       "\n",
       "<path stroke=\"black\" stroke-width=\"2\" d=\"M 400,100\" />'\n",
       "\n",
       "<path stroke=\"black\" stroke-width=\"2\" d=\"M 0,100 100,100 100,105 100,100 200,100 200,105 200,100 300,100 300,105 300,100 400,100 400,105 400,100 500,100 500,105 500,100 600,100 600,105 600,100 700,100 700,105 700,100 800,100\" />'\n",
       "\n",
       "<path stroke=\"orange\" stroke-width=\"2\" d=\"M 400,100\" />'\n",
       "\n",
       "<path stroke=\"orange\" stroke-width=\"2\" d=\"M 300,100 300,90\" />'\n",
       "\n",
       "<path stroke=\"green\" stroke-width=\"2\" d=\"M 400,100\" />'\n",
       "\n",
       "<path stroke=\"green\" stroke-width=\"2\" d=\"M 600,100 600,90\" />'\n",
       "\n",
       "<path stroke=\"blue\" stroke-width=\"2\" d=\"M 400,100\" />'\n",
       "\n",
       "<path stroke=\"blue\" stroke-width=\"2\" d=\"M 400,120 400.1,120 400.2,120 400.3,120 400.4,120 400.5,120 400.6,120 400.7,120 400.8,120 400.9,119.9 401,119.9 401.1,119.9 401.2,119.9 401.3,119.9 401.4,119.8 401.5,119.8 401.6,119.8 401.7,119.8 401.8,119.7 401.9,119.7 402,119.7 402.1,119.6 402.1,119.6 402.2,119.6 402.3,119.5 402.4,119.5 402.5,119.4 402.6,119.4 402.7,119.4 402.8,119.3 402.9,119.3 403,119.2 403,119.2 403.1,119.1 403.2,119 403.3,119 403.4,118.9 403.5,118.9 403.5,118.8 403.6,118.8 403.7,118.7 403.8,118.6 403.8,118.6 403.9,118.5 404,118.4 404.1,118.4 404.1,118.3 404.2,118.2 404.3,118.1 404.3,118.1 404.4,118 404.5,117.9 404.5,117.8 404.6,117.8 404.7,117.7 404.7,117.6 404.8,117.5 404.8,117.4 404.9,117.3 404.9,117.3 405,117.2 405,117.1 405.1,117 405.1,116.9 405.2,116.8 405.2,116.7 405.3,116.6 405.3,116.6 405.3,116.5 405.4,116.4 405.4,116.3 405.5,116.2 405.5,116.1 405.5,116 405.5,115.9 405.6,115.8 405.6,115.7 405.6,115.6 405.6,115.5 405.7,115.4 405.7,115.3 405.7,115.2 405.7,115.1 405.7,115 405.7,114.9 405.8,114.8 405.8,114.7 405.8,114.6 405.8,114.5 405.8,114.4 405.8,114.3 405.8,114.2 405.8,114.1 405.8,114 405.8,113.9 405.8,113.8 405.8,113.7 405.7,113.6 405.7,113.5 405.7,113.4 405.7,113.3 405.7,113.2 405.7,113.1 405.6,113 405.6,112.9 405.6,112.8 405.6,112.7 405.5,112.6 405.5,112.5 405.5,112.5 405.5,112.4 405.4,112.3 405.4,112.2 405.3,112.1 405.3,112 405.3,111.9 405.2,111.8 405.2,111.7 405.1,111.6 405.1,111.5 405,111.4 405,111.4 404.9,111.3 404.9,111.2 404.8,111.1 404.8,111 404.7,110.9 404.7,110.9 404.6,110.8 404.5,110.7 404.5,110.6 404.4,110.5 404.3,110.5 404.3,110.4 404.2,110.3 404.1,110.3 404.1,110.2 404,110.1 403.9,110 403.8,110 403.8,109.9 403.7,109.8 403.6,109.8 403.5,109.7 403.5,109.7 403.4,109.6 403.3,109.5 403.2,109.5 403.1,109.4 403,109.4 403,109.3 402.9,109.3 402.8,109.2 402.7,109.2 402.6,109.1 402.5,109.1 402.4,109.1 402.3,109 402.2,109 402.1,108.9 402.1,108.9 402,108.9 401.9,108.8 401.8,108.8 401.7,108.8 401.6,108.7 401.5,108.7 401.4,108.7 401.3,108.7 401.2,108.7 401.1,108.6 401,108.6 400.9,108.6 400.8,108.6 400.7,108.6 400.6,108.6 400.5,108.6 400.4,108.6 400.3,108.5 400.2,108.5 400.1,108.5 400,108.5 399.9,108.5 399.8,108.5 399.7,108.6 399.6,108.6 399.5,108.6 399.4,108.6 399.3,108.6 399.2,108.6 399.1,108.6 399,108.6 398.9,108.7 398.8,108.7 398.7,108.7 398.6,108.7 398.5,108.7 398.4,108.8 398.3,108.8 398.2,108.8 398.1,108.9 398,108.9 398,108.9 397.9,109 397.8,109 397.7,109.1 397.6,109.1 397.5,109.1 397.4,109.2 397.3,109.2 397.2,109.3 397.1,109.3 397.1,109.4 397,109.4 396.9,109.5 396.8,109.5 396.7,109.6 396.6,109.7 396.6,109.7 396.5,109.8 396.4,109.8 396.3,109.9 396.3,110 396.2,110 396.1,110.1 396,110.2 396,110.3 395.9,110.3 395.8,110.4 395.8,110.5 395.7,110.5 395.6,110.6 395.6,110.7 395.5,110.8 395.4,110.9 395.4,110.9 395.3,111 395.3,111.1 395.2,111.2 395.2,111.3 395.1,111.4 395.1,111.4 395,111.5 395,111.6 394.9,111.7 394.9,111.8 394.8,111.9 394.8,112 394.8,112.1 394.7,112.2 394.7,112.3 394.6,112.4 394.6,112.5 394.6,112.5 394.6,112.6 394.5,112.7 394.5,112.8 394.5,112.9 394.5,113 394.4,113.1 394.4,113.2 394.4,113.3 394.4,113.4 394.4,113.5 394.4,113.6 394.3,113.7 394.3,113.8 394.3,113.9 394.3,114 394.3,114.1 394.3,114.2 394.3,114.3 394.3,114.4 394.3,114.5 394.3,114.6 394.3,114.7 394.3,114.8 394.4,114.9 394.4,115 394.4,115.1 394.4,115.2 394.4,115.3 394.4,115.4 394.5,115.5 394.5,115.6 394.5,115.7 394.5,115.8 394.6,115.9 394.6,116 394.6,116.1 394.6,116.2 394.7,116.3 394.7,116.4 394.8,116.5 394.8,116.6 394.8,116.6 394.9,116.7 394.9,116.8 395,116.9 395,117 395.1,117.1 395.1,117.2 395.2,117.3 395.2,117.3 395.3,117.4 395.3,117.5 395.4,117.6 395.4,117.7 395.5,117.8 395.6,117.8 395.6,117.9 395.7,118 395.8,118.1 395.8,118.1 395.9,118.2 396,118.3 396,118.4 396.1,118.4 396.2,118.5 396.3,118.6 396.3,118.6 396.4,118.7 396.5,118.8 396.6,118.8 396.6,118.9 396.7,118.9 396.8,119 396.9,119 397,119.1 397.1,119.2 397.1,119.2 397.2,119.3 397.3,119.3 397.4,119.4 397.5,119.4 397.6,119.4 397.7,119.5 397.8,119.5 397.9,119.6 398,119.6 398,119.6 398.1,119.7 398.2,119.7 398.3,119.7 398.4,119.8 398.5,119.8 398.6,119.8 398.7,119.8 398.8,119.9 398.9,119.9 399,119.9 399.1,119.9 399.2,119.9 399.3,120 399.4,120 399.5,120 399.6,120 399.7,120 399.8,120 399.9,120 400,120\" />'\n",
       "\n",
       "<path stroke=\"blue\" stroke-width=\"2\" d=\"M 400,100\" />'\n",
       "\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc_width=800\n",
    "sc_height=200\n",
    "unit = 100\n",
    "numticks = (sc_width - unit) // unit\n",
    "\n",
    "jt.make_turtle(animate=False, height=sc_height, width=sc_width)\n",
    "jt.set_color(\"black\")\n",
    "jt.hide()\n",
    "\n",
    "# draw tickmark and return to origin facing east\n",
    "def draw_tick(num, color):\n",
    "    jt.set_color(color)\n",
    "    jt.jump_to(sc_width//2 + num*unit, sc_height//2)\n",
    "    jt.left(90)\n",
    "    jt.fd(unit/10)\n",
    "    jt.right(90)\n",
    "    jt.jump_to(sc_width//2, sc_height//2)\n",
    "    jt.draw()\n",
    "\n",
    "# draw small circle and return to origin facing east\n",
    "def draw_circle(num, color, offset=0):\n",
    "    jt.set_color(color)\n",
    "    jt.jump_to(sc_width//2 + num*unit, sc_height//2 + offset)\n",
    "    for i in range(360):\n",
    "        jt.forward(.1)\n",
    "        jt.left(1)\n",
    "    jt.jump_to(sc_width//2, sc_height//2)\n",
    "    jt.draw()\n",
    "\n",
    "# draw x-axis\n",
    "def draw_x_axis():\n",
    "    jt.set_color(\"black\")\n",
    "    jt.jump_to(sc_width//2 - 4*unit, sc_height//2)\n",
    "    for i in range(numticks):\n",
    "        jt.forward(unit)\n",
    "        jt.right(90)\n",
    "        jt.forward(unit//20)\n",
    "        jt.back(unit//20)\n",
    "        jt.left(90)\n",
    "    jt.forward(unit)\n",
    "    jt.jump_to(sc_width//2, sc_height//2)\n",
    "    jt.draw()\n",
    "    \n",
    "draw_x_axis() \n",
    "draw_tick(-1, \"orange\")\n",
    "draw_tick(2, \"green\")\n",
    "draw_circle(0, \"blue\", unit//5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f96077-2cc5-4a8b-8bdb-bce5f45c0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from University of Alberta Reinforcement Learning Coursera course.\n",
    "\n",
    "def argmax(q_values):\n",
    "    \"\"\"\n",
    "    Takes in a list of q_values and returns the index of the item \n",
    "    with the highest value. Breaks ties randomly.\n",
    "    returns: int - the index of the highest value in q_values\n",
    "    \"\"\"\n",
    "    top_value = float(\"-inf\")\n",
    "    ties = []\n",
    "    \n",
    "    for i in range(len(q_values)):\n",
    "        # if a value in q_values is greater than the highest value update top and reset ties to zero\n",
    "        # if a value is equal to top value add the index to ties\n",
    "        # return a random selection from ties.\n",
    "        # your code here\n",
    "        if q_values[i] > top_value:\n",
    "            ties = [i]\n",
    "            top_value = q_values[i]\n",
    "        elif q_values[i] == top_value:\n",
    "            ties.append(i)\n",
    "        \n",
    "    return np.random.choice(ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c249523-b32c-493b-b694-3b1961a1d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-armed bandit UCB agent\n",
    "\n",
    "class UCBAgentDG(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(5)\n",
    "        self.c = 0.0\n",
    "        \n",
    "    # note: main_agent.agent_init() now resets arm_count\n",
    "    def agent_init(self, agent_info={}):\n",
    "        super().agent_init(agent_info)\n",
    "        self.c = agent_info.get(\"c\", 0.0)\n",
    "\n",
    "    def agent_step(self, reward, observation=None, extra=None):\n",
    "        \"\"\"\n",
    "        Takes one step for the agent. It takes in a reward and observation and \n",
    "        returns the action the agent chooses at that time step.\n",
    "        \n",
    "        Arguments:\n",
    "        reward -- float, the reward the agent recieved from the environment after taking the last action.\n",
    "        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it\n",
    "                              until future lessons\n",
    "        Returns:\n",
    "        current_action -- int, the action chosen by the agent at the current time step.\n",
    "        \"\"\"\n",
    "        \n",
    "        a = self.last_action\n",
    "        self.arm_count[a] += 1\n",
    "        step_size = 1.0/self.arm_count[a]\n",
    "        self.q_values[a] = self.q_values[a] + step_size*(reward - self.q_values[a])\n",
    "        \n",
    "        t = np.sum(self.arm_count) + 1\n",
    "        cb = self.c * np.sqrt(np.log(t)/np.maximum(.1,self.arm_count))\n",
    "        \n",
    "        current_action = argmax(self.q_values + cb)        \n",
    "    \n",
    "        self.last_action = current_action\n",
    "        \n",
    "        return current_action\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd40d7c-da9e-4745-af09-53ae39d4c433",
   "metadata": {},
   "source": [
    "#### Run the UCBAgent\n",
    "\n",
    "Visualize the $q$-values and upper confidence bounds after every 5 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16308cb5-663d-4c01-861d-cc823ec065b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.76   0.40   0.98   2.24   1.87 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 657.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "# Fill in this UCBAgent experiment code to visualize the q-values and upper confidence bounds\n",
    "\n",
    "num_runs = 1                    # The number of times we run the experiment\n",
    "num_steps = 30                  # The number of pulls of each arm the agent takes\n",
    "np.random.seed(0)                 # For some reason successive runs give the same arms!?\n",
    "env = Environment()               # We set what environment we want to use to test\n",
    "ucbAgent = UCBAgentDG()       # We choose what agent we want to use\n",
    "env.env_init({})                # We pass the environment the information it needs. In this case nothing.\n",
    "best_possible = np.max(env.arms)\n",
    "best_arm = np.argmax(env.arms)\n",
    "pretty_print(env.arms)\n",
    "colors = [\"blue\", \"orange\", \"green\", \"red\", \"purple\"]\n",
    "\n",
    "ucbRewards = np.zeros((num_runs, num_steps))\n",
    "ucbOptimal = np.zeros((num_runs, num_steps))\n",
    "for run in tqdm(range(num_runs)):           # tqdm is what creates the progress bar below\n",
    "    np.random.seed(run)                     # the environment behaves differently each run\n",
    "    ucbAgent.agent_init({\"num_actions\": 5,    # greedy pessimistic agent with 10 arms\n",
    "                         \"initial_value\": 5.0})      \n",
    "    \n",
    "    ucbRlGlue = RLGlue(ucbAgent, env)      # new RLGlue experiment with the env and greedy agent\n",
    "    ucbRlGlue.rl_start()                         # We start the experiment\n",
    "        \n",
    "    for i in range(num_steps):\n",
    "        interaction = ucbRlGlue.rl_step()        # The environment and agent take a step and return\n",
    "        ucbRewards[run, i] = interaction.r    # the reward\n",
    "        ucbOptimal[run, i] = 100 if interaction.a == best_arm else 0\n",
    "\n",
    "    if i%5 == 0 and i > 0:\n",
    "        ## visualize the q-values and upper confidence bounds\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31750a37-f674-4b35-aa76-da1b14fcea05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gym]",
   "language": "python",
   "name": "conda-env-gym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
