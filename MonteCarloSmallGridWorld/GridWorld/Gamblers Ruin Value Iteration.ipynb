{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9af82a-4719-437f-ab2b-e92ff3766bc2",
   "metadata": {},
   "source": [
    "In Gambler's Ruin, the agent starts with a dollar amount $\\$s$ between $\\$1$ and $\\$99$.\n",
    "Each action is a wager between $0$ and $min(s, 100-s)$.\n",
    "The agent wins each wager with a fixed probability $0.4$.\n",
    "The state is the agent's dollar amount on hand, and the terminal states are $0$ and $100$.\n",
    "The reward for every action is $+1$ if the agent reaches $\\$100$ (wins a wager of amount $100-s$), and $0$ otherwise.\n",
    "The task is episodic and $\\gamma = 1$.\n",
    "\n",
    "This code executes Value Iteration:  \n",
    "Algorithm parameter: a small threshold $\\theta > 0$ determining accuracy of estimation.  \n",
    "Initialize $V(s)$, for all $s \\in S+$, arbitrarily except that $V(0) = V(100) = 0$.  \n",
    "Loop:  \n",
    "> $\\Delta \\leftarrow 0$  \n",
    "> Loop for each $s \\in S$:  \n",
    "> > $v \\leftarrow V(s)$  \n",
    "> > $V(s) \\leftarrow \\max_a \\sum_{s',r} p(s', r|s, a) (r + V(s'))$  \n",
    "> > $\\Delta \\leftarrow \\max(\\Delta, |v âˆ’ V(s)|$  \n",
    "until $\\Delta < \\theta$  \n",
    "Output a deterministic policy $\\pi = \\pi_*$, such that  \n",
    "$\\pi(s) = \\arg \\max_a \\sum_{s',r} p(s', r|s, a) (r + V(s'))$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b80ed40-8a3a-44fb-af17-660d4c7d1b89",
   "metadata": {},
   "source": [
    "## Value Iteration for Gambler's Ruin\n",
    "\n",
    "Reinforcement Learning October 14, 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a6f56-fb94-48e8-aa52-8d025c9b7213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# theta == threshold\n",
    "# gamma == discount factor\n",
    "# p_h = probability of heads\n",
    "\n",
    "# reward 1 when 100 is reached, 0 otherwise\n",
    "\n",
    "# actions = [0 to min(s, 100 - s)]\n",
    "\n",
    "\n",
    "def value_iteration(p_h=0.4, theta=0.0001, gamma=1):\n",
    "    # States: 0 to 100\n",
    "    states = np.arange(101)\n",
    "    V = np.zeros(101)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5524e2f-a2ed-472f-8d69-1695d3a53d33",
   "metadata": {},
   "source": [
    "#### Parameters - be sure to test on a small state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db884dcc-55fc-416a-90bf-5c641cf57539",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_AMOUNT = 10\n",
    "P = .4\n",
    "THETA = 0.01\n",
    "S = range(1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a6fb6-1899-4cea-9f72-4a88bda5ffaa",
   "metadata": {},
   "source": [
    "#### Function to update $V(s)$ for each $s \\in \\{1, \\ldots, 100\\}$\n",
    "\n",
    "**Optimization**\n",
    "This problem is small enough that you can get by with loops instead of numpy vectorization, but it's worth getting practice.\n",
    "1. When the agent has $\\$s$ and wagers $\\$a$, the expected return is $0.6*V(s-a) + 0.4*V(s+a)$.\n",
    "   The updated $V(s)$ is the max of this. You can vectorize this by making\n",
    "   $V(s-a)$ and $V(s+a)$ into two numpy arrays indexed by $a$. Warning: the indexing can get tricky.\n",
    "2. If you can further vectorize over the index $s$, more power to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfaff1b-b740-450c-9b1d-1e3a5a86db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Optimazing this:\n",
    "    delta = 0\n",
    "        for s in range(1, 100):\n",
    "            actions = range(1, min(s, 100 - s) + 1)\n",
    "            action_returns = []\n",
    "            for a in actions:\n",
    "                win = ph * (1 * (s + a >= 100) + (1 - (s + a >= 100)) * V[s + a])\n",
    "                lose = (1 - ph) * V[s - a]\n",
    "                action_returns.append(win + lose)\n",
    "            new_v = max(action_returns)\n",
    "            delta = max(delta, abs(new_v - V[s]))\n",
    "            V[s] = new_v\n",
    "\"\"\"\n",
    "\n",
    "def update_V(V):\n",
    "    \n",
    "    Del = 0.0\n",
    "\n",
    "    \n",
    "\n",
    "    return Del"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788964d6-aad4-42bf-82e6-2caf4a9459ed",
   "metadata": {},
   "source": [
    "#### Function to calculate the greedy $\\pi_*$ from $V(s)$ for each $s \\in \\{1, \\ldots, 100\\}$\n",
    "\n",
    "You can make `update_V()` update the current greedy $\\pi$, or you can implement the next function and run it after $V$ has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81f08a-abca-4d99-8cf6-e3b7cac0a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_pi(V):\n",
    "    pi = np.zeros(100) # initialize to wager 0\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a415ed-c383-493f-9206-82ab3319d783",
   "metadata": {},
   "source": [
    "#### Main loop - implements value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9299f202-0932-4a48-9480-363b065e7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize V\n",
    "V = np.zeros(101)\n",
    "    \n",
    "# Update V until Del < THETA\n",
    "Del = THETA\n",
    "while Del >= THETA:\n",
    "    Del = update_V(V)\n",
    "    \n",
    "# Calculate pi_*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3243f-2b1d-41e0-8ca2-37a6347541fd",
   "metadata": {},
   "source": [
    "#### Plot $V$ and $\\pi_*$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abdfe95-fa92-4d76-b4e6-69e47deda27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot V\n",
    "\n",
    "# plot pi_*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gym]",
   "language": "python",
   "name": "conda-env-gym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
